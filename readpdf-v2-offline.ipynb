{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizzel42\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Describe abstract of this document prova.pdf',\n",
       " 'context': [Document(page_content='promising amplified capabilities. However, ethical considera-\\ntions are paramount, emphasizing responsible development and\\ndeployment to ensure positive integration into our lives.\\nReferences\\n[Bahng et al. , 2022 ]Hyojin Bahng, Ali Jahanian, Swami\\nSankaranarayanan, and Phillip Isola. Exploring visual\\nprompts for adapting large-scale models. arXiv preprint\\narXiv:2203.17274 , 2022.\\n[Brown et al. , 2020 ]Tom B. Brown, Benjamin Mann, Nick\\nRyder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen\\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher Berner,\\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. Language models are few-shot learners, 2020.\\n[Chen et al. , 2022 ]Wenhu Chen, Xueguang Ma, Xinyi Wang,', metadata={'page': 7, 'source': 'prova.pdf'}),\n",
       "  Document(page_content='text generation. By integrating external tools for specialized\\nknowledge and computations, ART unlocks unprecedented\\nversatility and informs LLM outputs with real-world relevance.\\nThis allows LLMs to contribute to diverse fields like scientific\\nresearch, data analysis, and even decision-making support.', metadata={'page': 4, 'source': 'prova.pdf'}),\n",
       "  Document(page_content='CoC Manual Single text-davinci-003 ,gpt-3.5-turbo BIG-Bench Hard Accuracy\\nScratchpad Prompting Manual Single GPT-3 MBPP, MBPP-aug Accuracy\\nOptimization and\\nEfficiencyOPRO Manual Single PaLM 2-L-IT, text-bison GSM8K, BIG-Bench Hard Accuracy\\nUnderstanding\\nUser IntentRaR Manual Single GPT-4-061 3 Knowledge, SymbolicAccuray, Fair Score,\\nLanguage Modeling Score\\nMetacognition\\nand Self-ReflectionTake a Step Back Manual Single PaLM2-L, GPT-4MMLU-Physics, MMLU-Chemistry\\nTimeQA, SituatedQA, StrategyQAAccuracy\\nduce Optimization by PROmpting (OPRO), a novel approach\\nthat leverages LLMs as optimizers. Unlike traditional meth-\\nods, OPRO utilizes natural language prompts to iteratively\\ngenerate solutions based on the problem description, enabling\\nquick adaptation to different tasks and customization of the\\noptimization process. The potential of LLMs for optimization\\nis demonstrated through case studies on classic problems like\\nlinear regression and the traveling salesman problem. Addi-', metadata={'page': 6, 'source': 'prova.pdf'}),\n",
       "  Document(page_content='opening doors to a future brimming with possibilities. In\\nan ever-evolving landscape, ongoing research consistently re-\\nveals innovative approaches and applications within prompt\\nengineering. The significance of prompt engineering is under-\\nscored by its capacity to steer model responses, enhancing the\\nadaptability and applicability of LLMs across diverse sectors.\\nThe landscape of contemporary prompt engineering spans a\\nspectrum of techniques, encompassing foundational methods\\nlike zero-shot and few-shot prompting to more intricate ap-\\nproaches such as \"chain of code\" prompting. The notion of\\nprompt engineering was initially investigated and popularized\\nin the LLMs [Liuet al. , 2023 ],[Tonmoy et al. , 2024 ],[Chen\\net al. , 2023 ]later extended to VLMs [Wuet al. , 2023 ],[Bahng\\net al. , 2022 ]. Despite the extensive literature on prompt engi-\\nneering within both LLMs and VLMs, a notable gap remains,\\nparticularly concerning a systematic overview of application-', metadata={'page': 0, 'source': 'prova.pdf'})],\n",
       " 'answer': 'This document discusses advancements in large language models (LLMs) and visual prompting for their adaptation to various tasks. Ethical considerations are emphasized, along with the potential integration of external tools for specialized knowledge and computations, which unlocks unprecedented versatility and real-world relevance. The document also introduces a novel optimization approach called OPRO that utilizes LLMs as optimizers through natural language prompts for quick adaptation to tasks. The landscape of prompt engineering is explored, with its capacity to enhance the adaptability and applicability of LLMs across diverse sectors.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatMistralAI(endpoint=\"http://localhost:5050/v1/\", api_key=\"not-needed\")\n",
    "\n",
    "\n",
    "file_path = \"prova.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(splits, embedding_function)\n",
    "\n",
    "docs = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"input({input})\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "rag_chain = create_retrieval_chain(docs, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": input(\"inserisci la richiesta\")})\n",
    "\n",
    "results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
